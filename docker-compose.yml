version: '3'

services:
  zookeeper:
    image: wurstmeister/zookeeper:latest
    container_name: zookeeper
    networks:
      - kafka-net
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka-1:
    image: confluentinc/cp-kafka:7.2.2
    container_name: kafka-1
    networks:
      - kafka-net
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka-1:29092,EXTERNAL://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    depends_on:
      - zookeeper

  kafka-2:
    image: confluentinc/cp-kafka:7.2.2
    container_name: kafka-2
    networks:
      - kafka-net
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka-2:29093,EXTERNAL://localhost:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    depends_on:
      - zookeeper
      
  init-kafka:
    image: confluentinc/cp-kafka:7.2.2
    depends_on:
      - zookeeper
      - kafka-1
      - kafka-2
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      echo -e 'Creating kafka topics'
      kafka-topics --bootstrap-server kafka-1:9092 --create --if-not-exists --topic logs --replication-factor 2 --partitions 1
      kafka-topics --bootstrap-server kafka-2:9092 --create --if-not-exists --topic logs --replication-factor 2 --partitions 1
      "
    
  elasticsearch:
    container_name: elasticsearch
    image: elasticsearch:7.9.1
    environment:
      - cluster.name=kafka-cluster
      - bootstrap.memory_lock=true
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data:rw
    ulimits:
      memlock:
        soft: -1
        hard: -1
    depends_on:
      - kafka-1
      - kafka-2
    stdin_open: true
    tty: true
    networks:
      - kafka-net
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "50"
    healthcheck:
      test: curl -u elastic:elastic -s -f elasticsearch:9200/_cat/health >/dev/null || exit 1
      interval: 10s
      timeout: 10s
      retries: 5

  # postgres:
  #   container_name: postgres
  #   build:
  #     context: ./server/postgres/
  #     dockerfile: Dockerfile
  #   restart: always
  #   expose:
  #     - 5432
  #   environment:
  #     - POSTGRES_DB=postgres
  #     - POSTGRES_USER=postgres
  #     - POSTGRES_PASSWORD=password
  #     - POSTGRES_PORT=5432
  #   volumes:
  #     - postgresvolume:/var/lib/postgresql/data
  #   networks:
  #     - kafka-net

  logstash:
    container_name: logstash
    image: logstash:7.9.1
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - ./server/logstash/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
      - ./server/logstash/postgres.sh:/usr/share/logstash/scripts/postgres.sh
    depends_on:
      - elasticsearch
    stdin_open: true
    tty: true
    networks:
      - kafka-net
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "50"
    healthcheck:
      test: ["CMD", "curl", "--silent", "--fail", "http://logstash:9600"]
      interval: 30s
      timeout: 15s
      retries: 3

  kibana:
    container_name: kibana-cntr
    image: kibana:7.9.1
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
      - 5601:5601
    depends_on:
      - elasticsearch
    stdin_open: true
    tty: true
    networks:
      - kafka-net
    links: ['elasticsearch']
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "50"
    healthcheck:
      test: curl --fail http://kibana:5601 || exit 1
      interval: 30s
      retries: 3
      timeout: 10s

  query-service-1:
    container_name: query-service-1
    build:
      context: ./server/query_service
      dockerfile: Dockerfile
    environment:
      - PORT=3000
      - ELASTIC_ADDR=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    networks:
      - kafka-net
    restart: always

  query-service-2:
    container_name: query-service-2
    build:
      context: ./server/query_service
      dockerfile: Dockerfile
    environment:
      - PORT=3000
      - ELASTIC_ADDR=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    networks:
      - kafka-net
    restart: always

  query-service-nginx:
    image: nginx
    container_name: query-service-nginx
    ports:
      - "3001:80"
    volumes:
      - ./nginx/query-service.conf:/etc/nginx/nginx.conf
    depends_on:
      - query-service-1
      - query-service-2
    networks:
      - kafka-net
    restart: always
  
  log-producer-1:
    container_name: log-producer-1
    build:
      context: ./server/log_producer
      dockerfile: Dockerfile
    environment:
      - PORT=3000
      - KAFKA_ADDR=kafka-1:29092,kafka-2:29093
    depends_on:
      - kafka-1
      - kafka-2
    restart: always
    networks:
      - kafka-net

  log-producer-2:
    container_name: log-producer-2
    build:
      context: ./server/log_producer
      dockerfile: Dockerfile
    environment:
      - PORT=3000
      - KAFKA_ADDR=kafka-1:29092,kafka-2:29093
    restart: always
    depends_on:
      - kafka-1
      - kafka-2
    networks:
      - kafka-net

  log-producer-nginx:
    image: nginx
    container_name: log-producer-nginx
    ports:
      - "3000:80"
    restart: always
    volumes:
      - ./nginx/log-producer.conf:/etc/nginx/nginx.conf
    depends_on:
      - log-producer-1
      - log-producer-2
    networks:
      - kafka-net
  
  web-client:
    container_name: web-client
    build:
      context: ./client
      dockerfile: Dockerfile
    ports:
      - "4173:4173"


networks:
  kafka-net:
    driver: bridge

volumes:
  elasticsearch_data:
    driver: local
  postgresvolume:
    driver: local
